<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HDFS原理（]]></title>
    <url>%2F2019%2F06%2F26%2FHDFS%E5%8E%9F%E7%90%86%E5%8F%8A%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[HDFS原理及操作1、HDFS原理HDFS（Hadoop Distributed File System）是一个分布式文件系统，是谷歌的GFS山寨版本。它具有高容错性并提供了高吞吐量的数据访问，非常适合大规模数据集上的应用，它提供了一个高度容错性和高吞吐量的海量数据存储解决方案。 高吞吐量访问：HDFS的每个Block分布在不同的Rack上，在用户访问时，HDFS会计算使用最近和访问量最小的服务器给用户提供。由于Block在不同的Rack上都有备份，所以不再是单数据访问，所以速度和效率是非常快的。另外HDFS可以并行从服务器集群中读写，增加了文件读写的访问带宽。 高容错性：系统故障是不可避免的，如何做到故障之后的数据恢复和容错处理是至关重要的。HDFS通过多方面保证数据的可靠性，多份复制并且分布到物理位置的不同服务器上，数据校验功能、后台的连续自检数据一致性功能都为高容错提供了可能。 线性扩展：因为HDFS的Block信息存放到NameNode上，文件的Block分布到DataNode上，当扩充的时候仅仅添加DataNode数量，系统可以在不停止服务的情况下做扩充，不需要人工干预。 2、HDFS架构如上图所示HDFS是Master和Slave的结构，分为NameNode、Secondary NameNode和DataNode三种角色。NameNode：在Hadoop1.X中只有一个Master节点，管理HDFS的名称空间和数据块映射信息、配置副本策略和处理客户端请求； Secondary NameNode：辅助NameNode，分担NameNode工作，定期合并fsimage和fsedits并推送给NameNode，紧急情况下可辅助恢复NameNode； DataNode：Slave节点，实际存储数据、执行数据块的读写并汇报存储信息给NameNode； 3、HDFS读操作1、客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象时分布文件系统的一个实例； 2、DistributedFileSystem通过使用RPC来调用NameNode以确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面； 3、 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流，客户端对这个输入流调用read()方法； 4、存储着文件起始块的DataNode地址的DFSInputStream随即连接距离最近的DataNode，通过对数据流反复调用read()方法，可以将数据从DataNode传输到客户端； 5、 到达块的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode，这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流； 6、 一旦客户端完成读取，就对FSDataInputStream调用close()方法关闭文件读取 4、HDFS写操作1、客户端通过调用DistributedFileSystem的create()方法创建新文件； 2、DistributedFileSystem通过RPC调用NameNode去创建一个没有Blocks关联的新文件，创建前NameNode会做各种校验，比如文件是否存在、客户端有无权限去创建等。如果校验通过，NameNode会为创建新文件记录一条记录，否则就会抛出IO异常； 3、前两步结束后会返回FSDataOutputStream的对象，和读文件的时候相似，FSDataOutputStream被封装成DFSOutputStream，DFSOutputStream可以协调NameNode和Datanode。客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切成一个个小的数据包，并写入内部队列称为“数据队列”(Data Queue)； 4、DataStreamer会去处理接受Data Queue，它先问询NameNode这个新的Block最适合存储的在哪几个DataNode里，比如重复数是3，那么就找到3个最适合的DataNode，把他们排成一个pipeline.DataStreamer把Packet按队列输出到管道的第一个Datanode中，第一个DataNode又把Packet输出到第二个DataNode中，以此类推； 5、HFSOutputStream还有一个对列叫Ack Quene，也是有Packet组成，等待DataNode的收到响应，当Pipeline中的所有DataNode都表示已经收到的时候，这时Akc Quene才会把对应的Packet包移除掉； 6、客户端完成写数据后调用close()方法关闭写入流； 7、ataStreamer把剩余的包都刷到Pipeline里然后等待Ack信息，收到最后一个Ack后，通知NameNode把文件标示为已完成。 5、HDFS中常用到的命令lhadoop fs hadoop fs -ls / hadoop fs -lsr hadoop fs -mkdir /user/hadoop hadoop fs -put a.txt /user/hadoop/ hadoop fs -get /user/hadoop/a.txt / hadoop fs -cp src dst hadoop fs -mv src dst hadoop fs -cat /user/hadoop/a.txt hadoop fs -rm /user/hadoop/a.txt hadoop fs -rmr /user/hadoop/a.txt hadoop fs -text /user/hadoop/a.txt hadoop fs -copyFromLocal localsrc dst 与hadoop fs -put功能类似。 hadoop fs -moveFromLocal localsrc dst 将本地文件上传到hdfs，同时删除本地文件。 lhadoop fsadmin hadoop dfsadmin -report hadoop dfsadmin -safemode enter | leave | get | wait hadoop dfsadmin -setBalancerBandwidth 1000 lhadoop fsck lstart-balancer.sh 6、测试例子123456789101112131415161718192021import java.io.InputStream;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.apache.hadoop.io.IOUtils;public class FileSystemCat &#123; public static void main(String[] args) throws Exception &#123; String uri = args[0]; Configuration conf = new Configuration(); FileSystem fs = FileSystem. get(URI.create (uri), conf); InputStream in = null; try &#123; in = fs.open( new Path(uri)); IOUtils.copyBytes(in, System.out, 4096, false); &#125; finally &#123; IOUtils.closeStream(in); &#125; &#125;&#125; 6.1 创建代码目录使用如下命令启动Hadoop cd /app/hadoop-1.1.2/bin ./start-all.sh 在/app/hadoop-1.1.2目录下使用如下命令建立myclass和input目录：cd /app/hadoop-1.1.2mkdir myclassmkdir input 6.2 建立例子文件上传到HDFS中进入/app/hadoop-1.1.2/input目录，在该目录中建立quangle.txt文件cd /app/hadoop-1.1.2/inputtouch quangle.txtvi quangle.txt内容为：On the top of the Crumpetty Tree The Quangle Wangle sat,、 But his face you could not see, On account of his Beaver Hat.使用如下命令在hdfs中建立目录/class4hadoop fs -mkdir /class4hadoop fs -ls /把例子文件上传到hdfs的/class4文件夹中cd /app/hadoop-1.1.2/inputhadoop fs -copyFromLocal quangle.txt /class4/quangle.txthadoop fs -ls /class4 6.3 编译代码在/app/hadoop-1.1.2/myclass目录中，使用如下命令编译代码：javac -classpath ../hadoop-core-1.1.2.jar FileSystemCat.java]]></content>
      <categories>
        <category>Hadoop学习之路</category>
      </categories>
      <tags>
        <tag>Hadoop学习之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop搭建（单机）]]></title>
    <url>%2F2019%2F06%2F26%2FHadoop2.0X64%E4%BD%8D%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-2%2F</url>
    <content type="text"><![CDATA[Hadoop2.0X64位环境搭建1、搭建环境部署节点操作系统为CentOS，防火墙和SElinux禁用，创建了一个shiyanlou用户并在系统根目录下创建/app目录，用于存放Hadoop等组件运行包。因为该目录用于安装hadoop等组件程序，用户对hadoop必须赋予rwx权限（一般做法是root用户在根目录下创建/app目录，并修改该目录拥有者为shiyanlou(chown –R hadoop:hadoop /app）。 Hadoop搭建环境虚拟机操作系统： CentOS7.5 64位JDK：1.8.0_12 64位Hadoop：2.2.0 64位 2.部署Hadooop2.X2.1配置Hadoop环境下载并解压hadoop安装包,解压缩并移动到/app目录下`tar -xzf hadoop-2.2.0.tar.gz mv hadoop-2.2.0 /app` 2.2 在Hadoop目录下创建子目录在hadoop-2.2.0目录下创建tmp、name和data目录cd /app/hadoop-2.2.0mkdir tmpmkdir hdfsmkdir hdfs/namemkdir hdfs/data 2.3配置hadoop-env.sh 打开配置文件hadoop-env.sh cd /app/hadoop-2.2.0/etc/hadoop sudo vi hadoop-env.sh 加入配置内容，设置了hadoop中jdk和hadoop/bin路径export HADOOP_CONF_DIR=/app/hadoop2.2.0/etc/hadoopexport JAVA_HOME=/app/lib/jdk1.8.0_12export PATH=$PATH:/app/hadoop-2.2.0/bin 编译配置文件hadoop-env.sh，并确认生效source hadoop-env.shhadoop version 2.4配置yarn-env.sh打开配置文件yarn-env.sh，设置了hadoop中jdk路径，配置完毕后使用source yarn-env.sh编译该文件export JAVA_HOME=/app/lib/jdk1.8.0_12 2.5配置core-site.xml 使用如下命令打开core-site.xml配置文件cd /app/hadoop-2.2.0/etc/hadoopsudo vi core-site.xml 在配置文件中，按照如下内容进行配置 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/app/hadoop-2.2.0/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hduser.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;hadoop.proxyuser.hduser.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2.6配置hdfs-site.xml 使用如下命令打开hdfs-site.xml配置文件cd /app/hadoop-2.2.0/etc/hadoopsudo vi hdfs-site.xml 在配置文件中，按照如下内容进行配置 12345678910111213141516171819202122232425 &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/app/hadoop-2.2.0/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/app/hadoop-2.2.0/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2.7 配置mapred-site.xml1.默认情况下不存在mapred-site.xml文件，可以从模板拷贝一份，并使用如下命令打开mapred-site.xml配置文件cd /app/hadoop-2.2.0/etc/hadoopcp mapred-site.xml.template mapred-site.xmlsudo vi mapred-site.xml2.在配置文件中，按照如下内容进行配置 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2.8 配置yarn-site.xml1.使用如下命令打开yarn-site.xml配置文件cd /app/hadoop-2.2.0/etc/hadoopsudo vi yarn-site.xml2.在配置文件中，按照如下内容进行配置 123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;hadoop:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;hadoop:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;hadoop:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;hadoop:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2.9配置slaves文件在slaves配置文件中设置从节点，这里设置为hadoop，与Hadoop1.X区别的是Hadoop2.X不需要设置Mastercd /app/hadoop-2.2.0/etc/hadoopvi slaves 2.10格式化namenodecd /app/hadoop-2.2.0/bin./hdfs namenode -format 3启动Hadoop3.1启动hdfscd /app/hadoop-2.2.0/sbin./start-dfs.sh 3.2 验证当前进行cd /app/hadoop-2.2.0/sbin./start-yarn.sh 3.3 验证当前进行使用jps命令查看运行进程，此时在hadoop上运行的进程除了：namenode、secondarynamenode和datanode，增加了resourcemanager和nodemanager两个进程 4 测试Hadoop4.1创建测试目录cd /app/hadoop-2.2.0/bin./hadoop fs -mkdir -p /class3/input 准备测试数据./hadoop fs -copyFromLocal ../etc/hadoop/* /class3/input 4.2 运行wordcount例子cd /app/hadoop-2.2.0/bin./hadoop jar ../share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount /class3/input /class3/output 4.3 查看结果使用如下命令查看运行结果：./hadoop fs -ls /class3/output/./hadoop fs -cat /class3/output/part-r-00000 | less]]></content>
      <categories>
        <category>Hadoop学习之路</category>
      </categories>
      <tags>
        <tag>Hadoop学习助之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop简单实例]]></title>
    <url>%2F2019%2F06%2F18%2FHadoop%20--hdfs%20%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Hadoop –hdfs 简单操作前言引入Hadoop依赖包 123456&lt;!--添加Hadoop依赖包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; 引入cdh的仓库 123456&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; hdfs的基本操作命令说明hadoop常用命令：hadoop fs -ls / 查看指定目录下内容hadoop fs -put 将本地文件存储至hadoophadoop fs -copyFromLocal 本地文件系统复制文件到HDFS文件系统hadoop fs -moveFromLocal 本地文件系统移到文件到HDFS文件系统hadoop fs -cat 打开某个已存在文件hadoop fs -text 看指定目录下内容hadoop fs -get 将HDFS中的test.txt复制到本地文件系统中hadoop fs -mkdir 在hadoop指定目录内创建新目录hadoop fs -mv 移动/改名hadoop fs -getmerge 合并文件hadoop fs -rm 删除hadoop上指定文件hadoop fs -rmdir 删除hadoop上指定文件夹 代码实现创建HDFS文件夹1234567/** * 创建HDFS文件夹 */ @Test public void mkdir() throws Exception &#123; fileSystem.mkdirs(new Path(&quot;/hdfsapi/test&quot;)); &#125; 查看HDFS内容12345678/** * 查看HDFS内容 */ @Test public void text()throws Exception &#123; FSDataInputStream in = fileSystem.open(new Path(&quot;/cdh_version.properties&quot;)); IOUtils.copyBytes(in, System.out, 1024); &#125; 创建文件1234567891011/** * 创建文件 */ @Test public void create()throws Exception &#123;// FSDataOutputStream out = fileSystem.create(new Path(&quot;/hdfsapi/test/a.txt&quot;)); FSDataOutputStream out = fileSystem.create(new Path(&quot;/hdfsapi/test/b.txt&quot;)); out.writeUTF(&quot;hello pk: replication 1&quot;); out.flush(); out.close(); &#125; 文件名更改123456789101112/** * 测试文件名更改 * @throws Exception */ @Test public void rename() throws Exception &#123; Path oldPath = new Path(&quot;/hdfsapi/test/b.txt&quot;); Path newPath = new Path(&quot;/hdfsapi/test/c.txt&quot;); boolean result = fileSystem.rename(oldPath, newPath); System.out.println(result); &#125; 拷贝本地文件到HDFS文件系统123456789/** * 拷贝本地文件到HDFS文件系统 */ @Test public void copyFromLocalFile() throws Exception &#123; Path src = new Path(&quot;/Users/rocky/data/hello.txt&quot;); Path dst = new Path(&quot;/hdfsapi/test/&quot;); fileSystem.copyFromLocalFile(src,dst); &#125; 拷贝大文件到HDFS文件系统：带进度123456789101112131415@Test public void copyFromLocalBigFile() throws Exception &#123; InputStream in = new BufferedInputStream(new FileInputStream(new File(&quot;/Users/rocky/tmp/software/jdk-8u91-linux-x64.tar.gz&quot;))); FSDataOutputStream out = fileSystem.create(new Path(&quot;/hdfsapi/test/jdk.tgz&quot;), new Progressable() &#123; public void progress() &#123; System.out.print(&quot;.&quot;); &#125; &#125;); IOUtils.copyBytes(in, out ,4096); &#125; 拷贝HDFS文件到本地：下载123456@Test public void copyToLocalFile() throws Exception &#123; Path src = new Path(&quot;/hdfsapi/test/hello.txt&quot;); Path dst = new Path(&quot;/Users/rocky/tmp/software&quot;); fileSystem.copyToLocalFile(src, dst); &#125; 查看目标文件夹下的所有文件12345678910111213141516171819@Test public void listFiles() throws Exception &#123; FileStatus[] statuses = fileSystem.listStatus(new Path(&quot;/hdfsapi/test&quot;)); for(FileStatus file : statuses) &#123; String isDir = file.isDirectory() ? &quot;文件夹&quot; : &quot;文件&quot;; String permission = file.getPermission().toString(); short replication = file.getReplication(); long length = file.getLen(); String path = file.getPath().toString(); System.out.println(isDir + &quot;\t&quot; + permission + &quot;\t&quot; + replication + &quot;\t&quot; + length + &quot;\t&quot; + path ); &#125; &#125; 递归查看目标文件夹下的所有文件1234567891011121314151617181920@Test public void listFilesRecursive() throws Exception &#123; RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(new Path(&quot;/hdfsapi/test&quot;), true); while (files.hasNext()) &#123; LocatedFileStatus file = files.next(); String isDir = file.isDirectory() ? &quot;文件夹&quot; : &quot;文件&quot;; String permission = file.getPermission().toString(); short replication = file.getReplication(); long length = file.getLen(); String path = file.getPath().toString(); System.out.println(isDir + &quot;\t&quot; + permission + &quot;\t&quot; + replication + &quot;\t&quot; + length + &quot;\t&quot; + path ); &#125; &#125; 查看文件块信息12345678910111213public void getFileBlockLocations() throws Exception &#123; FileStatus fileStatus = fileSystem.getFileStatus(new Path(&quot;/hdfsapi/test/jdk.tgz&quot;)); BlockLocation[] blocks = fileSystem.getFileBlockLocations(fileStatus,0,fileStatus.getLen()); for(BlockLocation block : blocks) &#123; for(String name: block.getNames()) &#123; System.out.println(name +&quot; : &quot; + block.getOffset() + &quot; : &quot; + block.getLength() + &quot; : &quot; + block.getHosts()); &#125; &#125;&#125; 结语本实例提前配置完成Hadoop配置的，只是HDFS的基础使用]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop实例</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F14%2F%E6%B5%8B%E8%AF%95%E4%BD%BF%E7%94%A8%E5%8F%88%E6%8B%8D%E4%BA%91%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[测试使用又拍云上传图片测试 测试使用又拍云上传图片测试]]></content>
  </entry>
  <entry>
    <title><![CDATA[阿里代码规范要求避免使用Apache BeanUtils进行属性复]]></title>
    <url>%2F2019%2F06%2F13%2F%E9%98%BF%E9%87%8C%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83%E8%A6%81%E6%B1%82%E9%81%BF%E5%85%8D%E4%BD%BF%E7%94%A8Apache%20BeanUtils%E8%BF%9B%E8%A1%8C%E5%B1%9E%E6%80%A7%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[阿里代码规范要求避免使用Apache BeanUtils进行属性复制起因在一次开发过程中，刚好看到小伙伴在调用set方法，将数据库中查询出来的数据PO对象的属性拷贝到VO对象中。当PO和VO的两个对象的字段属性绝大部分是一样的，我们一个一个的set做了大量的重复工作，而且这种操作很容易出错，因为对象属性太多，有可能漏掉或者重复set肉眼很难发现。类似这种操作我们很容易想到可以通过反射解决。其实用一个BeanUtils工具类就可以搞定了。但是如果使用Apache的BeanUtils.copyPropreties进行属性拷贝，这就是一个坑 阿里代码规范当我们开启阿里的代码扫描插件时，如果使用Apache的BeanUtils.copyPropreties进行属性拷贝，它会给一个非常严重的警告。因为，Apache BeanUtils性能较差，可以使用 Spring BeanUtils 或者 Cglib BeanCopier 来代替。看到这样的警告，有点让人有点不爽。大名鼎鼎的 Apache 提供的包，居然会存在性能问题，以致于阿里给出了严重的警告。 性能问题究竟是有多严重毕竟，在我们的应用场景中，如果只是很微小的性能损耗，但是能带来非常大的便利性，还是可以接受的。 验证测试方法接口和实现定义 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117public interface PropertiesCopier &#123; void copyProperties(Object source, Object target) throws Exception;&#125;public class CglibBeanCopierPropertiesCopier implements PropertiesCopier &#123; @Override public void copyProperties(Object source, Object target) throws Exception &#123; BeanCopier copier = BeanCopier.create(source.getClass(), target.getClass(), false); copier.copy(source, target, null); &#125;&#125;// 全局静态 BeanCopier，避免每次都生成新的对象public class StaticCglibBeanCopierPropertiesCopier implements PropertiesCopier &#123; private static BeanCopier copier = BeanCopier.create(Account.class, Account.class, false); @Override public void copyProperties(Object source, Object target) throws Exception &#123; copier.copy(source, target, null); &#125;&#125;public class SpringBeanUtilsPropertiesCopier implements PropertiesCopier &#123; @Override public void copyProperties(Object source, Object target) throws Exception &#123; org.springframework.beans.BeanUtils.copyProperties(source, target); &#125;&#125;public class CommonsBeanUtilsPropertiesCopier implements PropertiesCopier &#123; @Override public void copyProperties(Object source, Object target) throws Exception &#123; org.apache.commons.beanutils.BeanUtils.copyProperties(target, source); &#125;&#125;public class CommonsPropertyUtilsPropertiesCopier implements PropertiesCopier &#123; @Override public void copyProperties(Object source, Object target) throws Exception &#123; org.apache.commons.beanutils.PropertyUtils.copyProperties(target, source); &#125;&#125; 单元测试然后写一个参数化的单元测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173@RunWith(Parameterized.class)public class PropertiesCopierTest &#123; @Parameterized.Parameter(0) public PropertiesCopier propertiesCopier; // 测试次数 private static List&lt;Integer&gt; testTimes = Arrays.asList(100, 1000, 10_000, 100_000, 1_000_000); // 测试结果以 markdown 表格的形式输出 private static StringBuilder resultBuilder = new StringBuilder(&quot;|实现|100|1,000|10,000|100,000|1,000,000|&quot;).append(&quot;|----|----|----|----|----|----|&quot;); @Parameterized.Parameters public static Collection&lt;Object[]&gt; data() &#123; Collection&lt;Object[]&gt; params = new ArrayList&lt;&gt;(); params.add(new Object[]&#123;new StaticCglibBeanCopierPropertiesCopier()&#125;); params.add(new Object[]&#123;new CglibBeanCopierPropertiesCopier()&#125;); params.add(new Object[]&#123;new SpringBeanUtilsPropertiesCopier()&#125;); params.add(new Object[]&#123;new CommonsPropertyUtilsPropertiesCopier()&#125;); params.add(new Object[]&#123;new CommonsBeanUtilsPropertiesCopier()&#125;); return params; &#125; @Before public void setUp() throws Exception &#123; String name = propertiesCopier.getClass().getSimpleName().replace(&quot;PropertiesCopier&quot;, &quot;&quot;); resultBuilder.append(&quot;|&quot;).append(name).append(&quot;|&quot;); &#125; @Test public void copyProperties() throws Exception &#123; Account source = new Account(1, &quot;test1&quot;, 30D); Account target = new Account(); // 预热一次 propertiesCopier.copyProperties(source, target); for (Integer time : testTimes) &#123; long start = System.nanoTime(); for (int i = 0; i &lt; time; i++) &#123; propertiesCopier.copyProperties(source, target); &#125; resultBuilder.append((System.nanoTime() - start) / 1_000_000D).append(&quot;|&quot;); &#125; resultBuilder.append(&quot;&quot;); &#125; @AfterClass public static void tearDown() throws Exception &#123; System.out.println(&quot;测试结果：&quot;); System.out.println(resultBuilder); &#125;&#125; 测试结果 实现 100 1,000 StaticCglibBeanCopier 0.0563361 0.680016 CglibBeanCopier 4.099259 12.252336 SpringBeanUitils 3.80229 9.268228 CommonsPropertyUtils 6,797116 20.59255 结果表明，Cglib 的 BeanCopier 的拷贝速度是最快的，即使是百万次的拷贝也只需要 10 毫秒！ 相比而言，最差的是 Commons 包的 BeanUtils.copyProperties 方法，100 次拷贝测试与表现最好的 Cglib 相差 400 倍之多 原因分析查看源码，我们会发现 CommonsBeanUtils 主要有以下几个耗时的地方： 1.输出了大量的日志调试信息2.重复的对象类型检查3.类型转换 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114public void copyProperties(final Object dest, final Object orig) throws IllegalAccessException, InvocationTargetException &#123; // 类型检查 if (orig instanceof DynaBean) &#123; ... &#125; else if (orig instanceof Map) &#123; ... &#125; else &#123; final PropertyDescriptor[] origDescriptors = ... for (PropertyDescriptor origDescriptor : origDescriptors) &#123; ... // 这里每个属性都调一次 copyProperty copyProperty(dest, name, value); &#125; &#125; &#125; public void copyProperty(final Object bean, String name, Object value) throws IllegalAccessException, InvocationTargetException &#123; ... // 这里又进行一次类型检查 if (target instanceof DynaBean) &#123; ... &#125; ... // 需要将属性转换为目标类型 value = convertForCopy(value, type); ... &#125; // 而这个 convert 方法在日志级别为 debug 的时候有很多的字符串拼接 public &lt;T&gt; T convert(final Class&lt;T&gt; type, Object value) &#123; if (log().isDebugEnabled()) &#123; log().debug(&quot;Converting&quot; + (value == null ? &quot;&quot; : &quot; &apos;&quot; + toString(sourceType) + &quot;&apos;&quot;) + &quot; value &apos;&quot; + value + &quot;&apos; to type &apos;&quot; + toString(targetType) + &quot;&apos;&quot;); &#125; ... if (targetType.equals(String.class)) &#123; return targetType.cast(convertToString(value)); &#125; else if (targetType.equals(sourceType)) &#123; if (log().isDebugEnabled()) &#123; log().debug(&quot;No conversion required, value is already a &quot; + toString(targetType)); &#125; return targetType.cast(value); &#125; else &#123; // 这个 convertToType 方法里也需要做类型检查 final Object result = convertToType(targetType, value); if (log().isDebugEnabled()) &#123; log().debug(&quot;Converted to &quot; + toString(targetType) + &quot; value &apos;&quot; + result + &quot;&apos;&quot;); &#125; return targetType.cast(result); &#125; &#125; 性能和源码分析推荐阅读几种copyProperties工具类性能比较：https://www.jianshu.com/p/bcbacab3b89e CGLIB中BeanCopier源码实现：https://www.jianshu.com/p/f8b892e08d26 Java Bean Copy框架性能对比：https://yq.aliyun.com/articles/392185]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>工作经验 java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决大量if判断]]></title>
    <url>%2F2019%2F06%2F13%2F%E4%BC%98%E9%9B%85%E8%A7%A3%E5%86%B3%E5%A4%A7%E9%87%8Fif%E5%88%A4%E6%96%AD%2F</url>
    <content type="text"><![CDATA[优雅解决大量if判断一：前言大家都在项目上遇到大量的if判断影响代码的可读性和复杂度，对于后期不好维护等问题。下面如何解决大量if条件让代码看起来很优雅 二：项目需求我在项目里有个集成支付接口，比如支付宝、微信、银联等支付接口。在前端入参的支付类型，后端根据支付类型调用封装好的支付接口，这样一来代码就会存在难以维护的if校验。 三:引入策略设计模式解决if判断项目中的实例1.创建一个interface接口 12345678910111213public interface RefundStrategy &#123; /** * * @param tradeNo 交易流水号 * @param refundNo 退款单号 * @param totalAmount 订单总金额 * @param refundAmount 退款金额 * @param refundReason 退款原因 * @return 退款结果集 &#123;@link PayResult&#125; */ PayResult doPayRefund(String tradeNo, String refundNo, String orderNo,String thirdpartPayNo, String totalAmount, String refundAmount, String refundReason);&#125; 2.在Ali实现类实现该接口1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Data@Componentpublic class AliPayRefundStrategy implements RefundStrategy &#123; private static YueProperties properties; private static CommonUtil commonUtil; private static void setCommonUtilFactory(CommonUtil util) &#123; if (AliPayRefundStrategy.commonUtil == null) &#123; AliPayRefundStrategy.commonUtil = util; &#125; &#125; @Resource public void setCommonUtil(CommonUtil util) &#123; AliPayRefundStrategy.setCommonUtilFactory(util); &#125; private static void setPropertiesFactory(YueProperties yueProperties) &#123; if (AliPayRefundStrategy.properties == null) &#123; AliPayRefundStrategy.properties = yueProperties; &#125; &#125; @Resource public void setProperties(YueProperties yueProperties) &#123; AliPayRefundStrategy.setPropertiesFactory(yueProperties); &#125; @Override public PayResult doPayRefund(String tradeNo, String refundNo,String orderNo,String thirdpartPayNo, String totalAmount, String refundAmount, String refundReason) &#123; YuePay yuepay = new YueAliPay(properties.getPay().getAlipay().getAppId(), properties.getPay().getAlipay().getPublicKey(), properties.getPay().getAlipay().getPrivateKey()); String response = yuepay.refund(tradeNo, refundNo, totalAmount, refundAmount, refundReason); PayResult payResult = commonUtil.readValue(response, PayResult.class); //获取退款交易流水号 @SuppressWarnings(&quot;unchecked&quot;) Map&lt;String, String&gt; map = commonUtil.readValue(payResult.getParams(), Map.class); payResult.setTradeNo(map.get(&quot;trade_no&quot;)); return payResult; &#125; 3.创建策略工厂12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class RefundStrategyFactory &#123; private static Map&lt;Integer, RefundStrategy&gt; originalMap= new HashMap&lt;&gt;(); private static Map&lt;Integer, RefundStrategy&gt; unOriginalMap = new HashMap&lt;&gt;(); //原路退款 static &#123; originalMap.put(ReceptionPaymentDict.RefundPayType.WXPAY_CARD.getValue(), new WxPayRefundStrategy()); originalMap.put(ReceptionPaymentDict.RefundPayType.WXPAY_H5.getValue(), new WxPayRefundStrategy()); originalMap.put(ReceptionPaymentDict.RefundPayType.ALIPAY.getValue(), new AliPayRefundStrategy()); originalMap.put(ReceptionPaymentDict.RefundPayType.ALIPAY_H5.getValue(), new AliPayRefundStrategy()); originalMap.put(ReceptionPaymentDict.RefundPayType.JHPAY.getValue(), new JuhePayRefundStrategy()); originalMap.put(ReceptionPaymentDict.RefundPayType.WXPAY_GZH.getValue(), new WxPayRefundStrategy()); originalMap.put(ReceptionPaymentDict.PayType.UNION_PAY.getValue(), new UnionPayRefundStrategy()); originalMap.put(ReceptionPaymentDict.PayType.BEST_PAY.getValue(), new BestPayRefundStrategy()); &#125; //手工退款 static &#123; unOriginalMap.put(ReceptionPaymentDict.RefundPayType.JHPAY.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.PayType.CASH.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.COMPANY_TRANSFER.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.SZ_ICBC.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.OTHER.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.ALIPAY.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.WXPAY_CARD.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.NB_ICBC.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.CQ_ICBC.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.GZ_ICBC.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.PayType.POS.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.PayType.BEST_PAY.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.WXPAY_CARD.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.WXPAY_H5.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.ALIPAY.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.ALIPAY_H5.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.JHPAY.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.RefundPayType.WXPAY_GZH.getValue(), new OtherPayRefundStrategy()); unOriginalMap.put(ReceptionPaymentDict.PayType.UNION_PAY.getValue(), new OtherPayRefundStrategy()); &#125; private RefundStrategyFactory() &#123; &#125; public static RefundStrategy getPayRefundStrategy(Integer key, Boolean original) &#123; if (original) &#123; return originalMap.get(key); &#125; return unOriginalMap.get(key); &#125;&#125; 4.创建上下文请求 1234567891011121314public class PayRefundContext &#123; @Resource private RefundStrategy payRefundStrategy; public PayRefundContext(RefundStrategy payRefundStrategy) &#123; this.payRefundStrategy = payRefundStrategy; &#125; public PayResult executePayRefundStrategy(String tradeNo, String refundNo,String orderNo, String thirdpartPayNo, String totalAmount, String refundAmount, String refundReason) &#123; return payRefundStrategy.doPayRefund(tradeNo, refundNo, orderNo, thirdpartPayNo, totalAmount, refundAmount, refundReason); &#125;&#125; 5.测试类12345678@Test public void executePayRefundStrategy() &#123; PayRefundContext payRefundContext = new PayRefundContext(RefundStrategyFactory.getPayRefundStrategy(18, true)); PayResult payResult = payRefundContext.executePayRefundStrategy( &quot;1111&quot;,&quot;1111&quot;,&quot;1111&quot;, &quot;1111&quot;,&quot;111&quot;,&quot;111&quot;,&quot;1111&quot; ); &#125;]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>工作经验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8日期操作]]></title>
    <url>%2F2019%2F06%2F12%2FJava8%E6%97%A5%E6%9C%9F%E6%97%B6%E9%97%B4%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Java8日期时间操作 一：简介在java8之前 日期时间的API一直被开发者诟病，包括：Java.util.Date是可变类型SimpleDateFormat非线程安全等问题。故此，Java8引入了一套全新的日期时间处理API，新的API基于ISO标准日历系统。 二:Java.util.Date不推荐使用Calendar类虽然有所改进，但仍有很多需要改进的地方，所以对于严肃的日期/时间工作，每个人都建议使用Joda-Time. Java 8带来了由JSR-310定义的Joda-Time启发的新java.time.* package，旧的日期/日历类。除了这个一般的缺陷(其中包括缺少一个时区组件以及在DateFormat中更好地处理日期格式化和无法使用非公历日历表示)的问题，有一些具体的问题，真的伤害Date类，包括年份与公元年年份偏移量为1900的事实。 日历有自己的问题，但即使早在JDK 1.1，显然java.util.Date不会削减它。即使日历是可以说是最差的JDK API，它已经采取到版本7尝试解决它。 三：Java8初识时间日期实例1//Clock时钟 Clock clock = Clock.systemDefaultZone(); long millis = clock.millis(); //获取指定时间点 用Instant Instant instant = clock.instant(); //也可以取Date Date legacyDate = Date.from(instant); 实例2//在新API中时区使用ZoneId来表示。 ZoneId.getAvailableZoneIds(); ZoneId zoneId1 = ZoneId.of(&quot;Europe/Berlin&quot;); ZoneId zoneId2 = ZoneId.of(&quot;Brazil/East&quot;); System.out.println(zoneId1.getRules()); 实例3/LocalTime 本地时间 LocalTime localTime = LocalTime.now(zoneId1); LocalTime localTime1 = LocalTime.now(zoneId2); long hours = ChronoUnit.HOURS.between(localTime1,localTime); System.out.println(hours); 实例4//LocalTime 提供了多种工厂方法来简化对象的创建，包括解析时间字符串。 LocalTime localTime2= LocalTime.of(23, 59, 59); System.out.println(localTime);// 23:59:59 DateTimeFormatter germanFormatte = DateTimeFormatter.ofLocalizedTime(FormatStyle.SHORT).withLocale(Locale.GERMAN); String leetTime = localTime2.format(germanFormatte).toString(); System.out.println(leetTime); 四：相关API说明 == nstant 时间戳Duration 持续时间、时间差LocalDate 只包含日期，比如：2018-09-24LocalTime 只包含时间，比如：10:32:10LocalDateTime 包含日期和时间，比如：2018-09-24 10:32:10Peroid 时间段ZoneOffset 时区偏移量，比如：+8:00ZonedDateTime 带时区的日期时间Clock 时钟，可用于获取当前时间戳 java.time.format.DateTimeFormatter 时间格式化类 五：拓展 https://docs.oracle.com/javase/8/docs/api/]]></content>
      <categories>
        <category>JDK8新特性</category>
      </categories>
      <tags>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java11 --HttpClient]]></title>
    <url>%2F2019%2F06%2F12%2FJDK11%E6%96%B0%E7%89%B9%E6%80%A7%20-httpClient%2F</url>
    <content type="text"><![CDATA[JDK11新特性 –httpClient一:HttpURLConnenction目前存在的问题1.其基类 URLConnection 当初是设计为支持多协议，但其中大多已经成为非主流（ftp, gopher…）2.API 的设计早于 HTTP/1.1，过度抽象3.难以使用，存在许多没有文档化的行为4.它只支持阻塞模式（每个请求 / 响应占用一个线程） 二: HttpClient简介httpclient是Apache Jakarta common下的子项目，用来提供高效的、最新的、更能丰富的支持http协议的客户端编程工具包，并且它支持http协议最新的版本和建议。httpclient已经应用在很多项目中，比如Apache Jakarta上很著名的两个开源项目cactus和httplunit都使用了httpclient。 三:HttpClient特性1.以可扩展的面向对象的结构实现了HTTP全部的方法（GET、POST、put、delete、head、options、trace）。 2.支持HTTPS协议。 3.通过HTTP代理建立透明的连接。4.连接管理器支持多线程应用。支持设置最大连接数，同事支持设置每个主机的最大连接数，发现并关闭过期的连接。 5.自动处理Set-Cookie中的Cookie。 6.插件式的自定义Cookie策略。 7.request的输出流可以避免流中内容直接缓冲到socket服务器。 8.Response的输入流可以有效的从socket服务器直接读取相应内容 三:使用方法使用HttpClient发送请求、接收响应很简单，一般需要如下几步即可。1、创建httpclient对象。2、创建请求方法的实例，并制定请求url。如果需要发送get请求，创建httpclient对象；如果需要发送post请求，创建httpPOST对象。3、如果需要发送请求参数，可调用httpget、httpPost共同的setparams(HetpParams params)方法来添加请求参数；对于HttpPost对象而言，也可调用setEntity(HttpEntity entity)方法来设置请求参数。4、调用HttpClient对象的execute(HttpUriRequest request)发送请求，该方法返回一个HttpResponse。5、调用HttpResponse的getAllHeaders()、getHeaders(String name)等方法可获取服务器的响应头；调用HttpResponse的getEntity()方法可获取HttpEntity对象，该对象包装了服务器的响应内容。程序可通过该对象获取服务器的响应内容。6、释放连接。无论执行方法是否成功，都必须释放连接 四:代码实例实例1:同步get请求123456789public void syncGet() throws InterruptedException, IOException &#123; HttpClient client = HttpClient.newHttpClient(); HttpRequest request = HttpRequest.newBuilder().uri(URI.create(&quot;https://www.baidu.com&quot;)).build(); HttpResponse&lt;String&gt; response = client.send(request, HttpResponse.BodyHandlers.ofString()); System.out.println(response.body());&#125; 实例2:异步Get 请求void asyncGet() throws IOException, InterruptedException, ExecutionException &#123;1234567 HttpClient client = HttpClient.newHttpClient(); HttpRequest request = HttpRequest.newBuilder().uri(URI.create(&quot;https://www.baidu.com&quot;)).build(); CompletableFuture&lt;String&gt; result = client.sendAsync(request, HttpResponse.BodyHandlers.ofString()).thenApply(HttpResponse::body); System.out.println(result.get());&#125; 实例3 Post请求void testPostForm() throws IOException, InterruptedException &#123;12345678910 HttpClient client = HttpClient.newBuilder().build(); HttpRequest request = HttpRequest.newBuilder() .uri(URI.create(&quot;http://www.w3school.com.cn/demo/demo_form.asp&quot;)) .header(&quot;Content-Type&quot;,&quot;application/x-www-form-urlencoded&quot;) .POST(HttpRequest.BodyPublishers.ofString(&quot;name1=value1&amp;name2=value2&quot;)) .build(); HttpResponse&lt;String&gt; response = client.send(request, HttpResponse.BodyHandlers.ofString()); System.out.println(response.statusCode());&#125; 实例4:JSON传参 header指定内容是表单类型，然后通过BodyPublishers.ofString传递表单数据，需要自己构建表单参数12345678910111213141516171819202122232425 ObjectMapper objectMapper = new ObjectMapper(); StockDto dto = new StockDto(); dto.setName(&quot;hj&quot;); dto.setSymbol(&quot;hj&quot;); dto.setType(StockDto.StockType.SH); String requestBody = objectMapper .writerWithDefaultPrettyPrinter() .writeValueAsString(dto); HttpRequest request = HttpRequest.newBuilder(URI.create(&quot;http://localhost:8080/json/demo&quot;)) .header(&quot;Content-Type&quot;, &quot;application/json&quot;) .POST(HttpRequest.BodyPublishers.ofString(requestBody)) .build(); CompletableFuture&lt;StockDto&gt; result = HttpClient.newHttpClient() .sendAsync(request, HttpResponse.BodyHandlers.ofString()) .thenApply(HttpResponse::body) .thenApply(body -&gt; &#123; try &#123; return objectMapper.readValue(body,StockDto.class); &#125; catch (IOException e) &#123; return new StockDto(); &#125; &#125;);StockDtoSystem.out.println(result.get()); 实例5:并发请求12345678910111213141516171819202122232425262728//endAsync方法返回的是CompletableFuture，可以方便地进行转换、组合等操作//这里使用CompletableFuture.allOf组合在一起，最后调用join等待所有future完成public void testConcurrentRequests()&#123; HttpClient client = HttpClient.newHttpClient(); List&lt;String&gt; urls = List.of(&quot;http://www.baidu.com&quot;,&quot;http://www.alibaba.com/&quot;,&quot;http://www.tencent.com&quot;); List&lt;HttpRequest&gt; requests = urls.stream() .map(url -&gt; HttpRequest.newBuilder(URI.create(url))) .map(reqBuilder -&gt; reqBuilder.build()) .collect(Collectors.toList()); List&lt;CompletableFuture&lt;HttpResponse&lt;String&gt;&gt;&gt; futures = requests.stream() .map(request -&gt; client.sendAsync(request, HttpResponse.BodyHandlers.ofString())) .collect(Collectors.toList()); futures.stream() .forEach(e -&gt; e.whenComplete((resp,err) -&gt; &#123; if(err != null)&#123; err.printStackTrace(); &#125;else&#123; System.out.println(resp.body()); System.out.println(resp.statusCode()); &#125; &#125;)); CompletableFuture.allOf(futures .toArray(CompletableFuture&lt;?&gt;[]::new)) .join(); &#125; 六:总结HttpClient中常用到的类HttpClient |– DefaultHttpClient 构造方法： DefaultHttpClient 主要方法： HttpResponse execute(HttpUriRequest request)HttpUriRequest |– HttpGet 构造方法： HttpGet() HttpGet(String uri) |– HttpPost 构造方法： HttpPost(String uri) 主要方法： void setEntity(HttpEntity entity)HttpResponse 主要方法： StatusLine getStatusLine() Header[] getAllHeaders(); HttpEntity getEntity();HttpEntity 主要方法： InputStream getContent(); long getContentLength(); Header getContentType(); |– UrlEncodedFormEntity 构造方法：UrlEncodedFormEntity(List&lt;? extends NameValuePair&gt; params) //用于向请求对象中写入请求实体（包含请求参数（NameValuePair））EntityUtils public static byte[] toByteArray(HttpEntity entity) public static String toString(HttpEntity entity) public static String toString(HttpEntity entity , String encoding)StatusLine int getStatusCode()HttpStatus SC_OK SC_NOT_FOUNDHeader String getName() String getValue()NameValuePair String getName() String getValue() |– BasicNameValuePair 构造方法：BasicNameValuePair(String name , String value)]]></content>
      <categories>
        <category>jdk11新特性</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F11%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[单例模型定义：保证一个类仅有一个实例，并且提供一个全局访问点类型：创建型 单例-使用场景​ 想确保任何情况下都绝对只有一个实例 优点在内存里只有一个实例，减少了内存开销 可以避免对资源的多重占用 设置全局访问点，严格控制访问 缺点没有接口，需要扩展代码 单例-懒汉型]]></content>
  </entry>
</search>
