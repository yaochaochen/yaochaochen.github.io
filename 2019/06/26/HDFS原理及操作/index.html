<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="HDFS原理（"><meta name="keywords" content="Hadoop学习之路"><meta name="author" content="yaochaochen,undefined"><meta name="copyright" content="yaochaochen"><title>HDFS原理（【来自远方的程序猿】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"local_search.hits_empty"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS原理及操作"><span class="toc-number">1.</span> <span class="toc-text">HDFS原理及操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、HDFS原理"><span class="toc-number">1.1.</span> <span class="toc-text">1、HDFS原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、HDFS架构"><span class="toc-number">1.2.</span> <span class="toc-text">2、HDFS架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、HDFS读操作"><span class="toc-number">1.3.</span> <span class="toc-text">3、HDFS读操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、HDFS写操作"><span class="toc-number">1.4.</span> <span class="toc-text">4、HDFS写操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5、HDFS中常用到的命令"><span class="toc-number">1.5.</span> <span class="toc-text">5、HDFS中常用到的命令</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6、测试例子"><span class="toc-number">1.6.</span> <span class="toc-text">6、测试例子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-创建代码目录"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 创建代码目录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-建立例子文件上传到HDFS中"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 建立例子文件上传到HDFS中</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-编译代码"><span class="toc-number">1.6.3.</span> <span class="toc-text">6.3 编译代码</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">yaochaochen</div><div class="author-info-description">个人站，主要涉后端知识共享、实践教程、前沿技术共同学习等方面</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/yaochaochen" target="_blank">GitHub<i class="icon-dot bg-color2"></i></a><a class="links-button button-hover" href="mailto:ycc_ysoft@163.com" target="_blank">E-Mail<i class="icon-dot bg-color7"></i></a><a class="links-button button-hover" href="https://www.jianshu.com/u/8779d34f3a27" target="_blank">简书<i class="icon-dot bg-color4"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">10</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">7</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">5</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">来自远方的程序猿</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">HDFS原理（</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2019-06-26 | 更新于 2019-06-26</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Hadoop学习之路/">Hadoop学习之路</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Hadoop学习之路/">Hadoop学习之路</a></div></div></div><div class="main-content"><h1 id="HDFS原理及操作"><a href="#HDFS原理及操作" class="headerlink" title="HDFS原理及操作"></a>HDFS原理及操作</h1><h2 id="1、HDFS原理"><a href="#1、HDFS原理" class="headerlink" title="1、HDFS原理"></a>1、HDFS原理</h2><p>HDFS（Hadoop Distributed File System）是一个分布式文件系统，是谷歌的GFS山寨版本。它具有高容错性并提供了高吞吐量的数据访问，非常适合大规模数据集上的应用，它提供了一个高度容错性和高吞吐量的海量数据存储解决方案。</p>
<p><strong>高吞吐量访问</strong>：HDFS的每个Block分布在不同的Rack上，在用户访问时，HDFS会计算使用最近和访问量最小的服务器给用户提供。由于Block在不同的Rack上都有备份，所以不再是单数据访问，所以速度和效率是非常快的。另外HDFS可以并行从服务器集群中读写，增加了文件读写的访问带宽。</p>
<p><strong>高容错性</strong>：系统故障是不可避免的，如何做到故障之后的数据恢复和容错处理是至关重要的。HDFS通过多方面保证数据的可靠性，多份复制并且分布到物理位置的不同服务器上，数据校验功能、后台的连续自检数据一致性功能都为高容错提供了可能。</p>
<p><strong>线性扩展</strong>：因为HDFS的Block信息存放到NameNode上，文件的Block分布到DataNode上，当扩充的时候仅仅添加DataNode数量，系统可以在不停止服务的情况下做扩充，不需要人工干预。</p>
<h2 id="2、HDFS架构"><a href="#2、HDFS架构" class="headerlink" title="2、HDFS架构"></a>2、HDFS架构</h2><p><img src="hexo-imgs.test.upcdn.net/2019/06/26/15615296312137.jpg" alt><br>如上图所示HDFS是Master和Slave的结构，分为NameNode、Secondary NameNode和DataNode三种角色。<br><strong>NameNode：</strong>在Hadoop1.X中只有一个Master节点，管理HDFS的名称空间和数据块映射信息、配置副本策略和处理客户端请求；</p>
<p><strong>Secondary NameNode：</strong>辅助NameNode，分担NameNode工作，定期合并fsimage和fsedits并推送给NameNode，紧急情况下可辅助恢复NameNode；</p>
<p><strong>DataNode</strong>：Slave节点，实际存储数据、执行数据块的读写并汇报存储信息给NameNode；</p>
<h2 id="3、HDFS读操作"><a href="#3、HDFS读操作" class="headerlink" title="3、HDFS读操作"></a>3、HDFS读操作</h2><p><img src="!%5Bread%5D(media/15615295187505/222.gif)" alt><br>1、客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象时分布文件系统的一个实例；</p>
<p>2、DistributedFileSystem通过使用RPC来调用NameNode以确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面；</p>
<p>3、 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流，客户端对这个输入流调用read()方法；</p>
<p>4、存储着文件起始块的DataNode地址的DFSInputStream随即连接距离最近的DataNode，通过对数据流反复调用read()方法，可以将数据从DataNode传输到客户端；</p>
<p>5、 到达块的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode，这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流；</p>
<p>6、 一旦客户端完成读取，就对FSDataInputStream调用close()方法关闭文件读取</p>
<h2 id="4、HDFS写操作"><a href="#4、HDFS写操作" class="headerlink" title="4、HDFS写操作"></a>4、HDFS写操作</h2><p><img src="hexo-imgs.test.upcdn.net/2019/06/26/15615299548777.jpg" alt><br>1、客户端通过调用DistributedFileSystem的create()方法创建新文件；</p>
<p>2、DistributedFileSystem通过RPC调用NameNode去创建一个没有Blocks关联的新文件，创建前NameNode会做各种校验，比如文件是否存在、客户端有无权限去创建等。如果校验通过，NameNode会为创建新文件记录一条记录，否则就会抛出IO异常；</p>
<p>3、前两步结束后会返回FSDataOutputStream的对象，和读文件的时候相似，FSDataOutputStream被封装成DFSOutputStream，DFSOutputStream可以协调NameNode和Datanode。客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切成一个个小的数据包，并写入内部队列称为“数据队列”(Data Queue)；</p>
<p>4、DataStreamer会去处理接受Data Queue，它先问询NameNode这个新的Block最适合存储的在哪几个DataNode里，比如重复数是3，那么就找到3个最适合的DataNode，把他们排成一个pipeline.DataStreamer把Packet按队列输出到管道的第一个Datanode中，第一个DataNode又把Packet输出到第二个DataNode中，以此类推；</p>
<p>5、HFSOutputStream还有一个对列叫Ack Quene，也是有Packet组成，等待DataNode的收到响应，当Pipeline中的所有DataNode都表示已经收到的时候，这时Akc Quene才会把对应的Packet包移除掉；</p>
<p>6、客户端完成写数据后调用close()方法关闭写入流；</p>
<p>7、ataStreamer把剩余的包都刷到Pipeline里然后等待Ack信息，收到最后一个Ack后，通知NameNode把文件标示为已完成。</p>
<h2 id="5、HDFS中常用到的命令"><a href="#5、HDFS中常用到的命令" class="headerlink" title="5、HDFS中常用到的命令"></a>5、HDFS中常用到的命令</h2><p>lhadoop fs</p>
<p>hadoop fs -ls /</p>
<p>hadoop fs -lsr</p>
<p>hadoop fs -mkdir /user/hadoop</p>
<p>hadoop fs -put a.txt /user/hadoop/</p>
<p>hadoop fs -get /user/hadoop/a.txt /</p>
<p>hadoop fs -cp src dst</p>
<p>hadoop fs -mv src dst</p>
<p>hadoop fs -cat /user/hadoop/a.txt</p>
<p>hadoop fs -rm /user/hadoop/a.txt</p>
<p>hadoop fs -rmr /user/hadoop/a.txt</p>
<p>hadoop fs -text /user/hadoop/a.txt</p>
<p>hadoop fs -copyFromLocal localsrc dst 与hadoop fs -put功能类似。</p>
<p>hadoop fs -moveFromLocal localsrc dst 将本地文件上传到hdfs，同时删除本地文件。</p>
<p>lhadoop fsadmin</p>
<p>hadoop dfsadmin -report</p>
<p>hadoop dfsadmin -safemode enter | leave | get | wait</p>
<p>hadoop dfsadmin -setBalancerBandwidth 1000</p>
<p>lhadoop fsck</p>
<p>lstart-balancer.sh</p>
<h2 id="6、测试例子"><a href="#6、测试例子" class="headerlink" title="6、测试例子"></a>6、测试例子</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import java.io.InputStream;</span><br><span class="line"></span><br><span class="line">import java.net.URI;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.*;</span><br><span class="line">import org.apache.hadoop.io.IOUtils;</span><br><span class="line"></span><br><span class="line">public class FileSystemCat &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        String uri = args[0];</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        FileSystem fs = FileSystem. get(URI.create (uri), conf);</span><br><span class="line">        InputStream in = null;</span><br><span class="line">    try &#123;</span><br><span class="line">            in = fs.open( new Path(uri));</span><br><span class="line">            IOUtils.copyBytes(in, System.out, 4096, false);</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            IOUtils.closeStream(in);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-1-创建代码目录"><a href="#6-1-创建代码目录" class="headerlink" title="6.1 创建代码目录"></a>6.1 创建代码目录</h3><p>使用如下命令启动Hadoop</p>
<p>cd /app/hadoop-1.1.2/bin</p>
<p>./start-all.sh</p>
<p>在/app/hadoop-1.1.2目录下使用如下命令建立myclass和input目录：<br>cd /app/hadoop-1.1.2<br>mkdir myclass<br>mkdir input</p>
<h3 id="6-2-建立例子文件上传到HDFS中"><a href="#6-2-建立例子文件上传到HDFS中" class="headerlink" title="6.2 建立例子文件上传到HDFS中"></a>6.2 建立例子文件上传到HDFS中</h3><p>进入/app/hadoop-1.1.2/input目录，在该目录中建立quangle.txt文件<br>cd /app/hadoop-1.1.2/input<br>touch quangle.txt<br>vi quangle.txt<br>内容为：<br><code>On the top of the Crumpetty Tree
The Quangle Wangle sat,、
But his face you could not see,
On account of his Beaver Hat.</code><br>使用如下命令在hdfs中建立目录/class4<br>hadoop fs -mkdir /class4<br>hadoop fs -ls /<br>把例子文件上传到hdfs的/class4文件夹中<br>cd /app/hadoop-1.1.2/input<br>hadoop fs -copyFromLocal quangle.txt /class4/quangle.txt<br>hadoop fs -ls /class4</p>
<h3 id="6-3-编译代码"><a href="#6-3-编译代码" class="headerlink" title="6.3 编译代码"></a>6.3 编译代码</h3><p>在/app/hadoop-1.1.2/myclass目录中，使用如下命令编译代码：<br>javac -classpath ../hadoop-core-1.1.2.jar FileSystemCat.java</p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">yaochaochen</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/06/26/HDFS原理及操作/">http://yoursite.com/2019/06/26/HDFS原理及操作/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">来自远方的程序猿</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2019/06/26/Hadoop集群搭建/"><i class="fas fa-angle-left">&nbsp;</i><span>Hadoop集群搭建</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2019/06/26/Hadoop2.0X64位环境搭建-2/"><span>Hadoop搭建（单机）</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2019 By yaochaochen</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618},"display":{"superSample":2,"width":150,"height":300,"position":"left","hOffset":0,"vOffset":-20},"mobile":{"show":false},"react":{"opacityDefault":0.5,"pacityOnHover":0},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>